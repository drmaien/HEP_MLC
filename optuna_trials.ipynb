{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the combined dataset\n",
    "df = pd.read_csv('dataset.tsv', sep='\\t')\n",
    "feature_cols = ['mH2', 'mHD', 'mAD', 'mHDp', 'alpha', 'L2', 'L8', 'vs', 'm22sq']\n",
    "label_cols = ['valid_BFB', 'valid_Uni', 'valid_STU', 'valid_Higgs']\n",
    "\n",
    "X_selected = df[feature_cols].copy()\n",
    "y = df[label_cols]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(X, apply_yj=False, apply_scaler=False):\n",
    "    if apply_yj:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        X = pt.fit_transform(X)\n",
    "    if apply_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "# Custom metrics with correct casting to avoid float/int issues\n",
    "def subset_accuracy(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.cast(tf.reduce_all(tf.equal(tf.round(y_true), tf.round(y_pred)), axis=1), tf.float32))\n",
    "\n",
    "def hamming_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.cast(tf.not_equal(y_true, tf.round(y_pred)), tf.float32))\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    tn = tf.reduce_sum(tf.cast((1 - y_true) * (1 - y_pred), tf.float32))\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32))\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32))\n",
    "\n",
    "    numerator = tp * tn - fp * fn\n",
    "    denominator = tf.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + tf.keras.backend.epsilon())\n",
    "\n",
    "def macro_f1_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32), axis=0)\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32), axis=0)\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "\n",
    "    return tf.reduce_mean(f1)\n",
    "\n",
    "# Custom objective function for Optuna\n",
    "def custom_objective(val_subset_accuracy, val_loss, val_hamming_loss, val_mcc, val_f1_score):\n",
    "    inv_val_loss = 1 / (val_loss + 1e-8)\n",
    "    normalized_score = (\n",
    "        (0.4 * val_subset_accuracy) +\n",
    "        (0.4 * (inv_val_loss / (1 + inv_val_loss))) +  # Normalize to [0, 1]\n",
    "        (0.0 * (1 - val_hamming_loss)) +  # Invert so higher is better\n",
    "        (0.2 * ((val_mcc + 1) / 2)) +  # Normalize from [-1, 1] to [0, 1]\n",
    "        (0.0 * val_f1_score)\n",
    "    )\n",
    "    return normalized_score\n",
    "\n",
    "# Define the model creation function\n",
    "def create_model(trial_or_params, input_shape, num_labels):\n",
    "    if isinstance(trial_or_params, Trial):\n",
    "        trial = trial_or_params\n",
    "        n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "        units = [trial.suggest_int(f'n_units_{i}', 128, 1024) for i in range(n_layers)]\n",
    "        activation = trial.suggest_categorical('activation', ['relu', 'leaky_relu'])\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "        apply_batch_norm = trial.suggest_categorical('apply_batch_norm', [True, False])\n",
    "        optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'nadam'])\n",
    "        regularization = trial.suggest_categorical('regularization', [None, 'l2'])\n",
    "        reg_lambda = trial.suggest_float('reg_lambda', 1e-5, 1e-2) if regularization == 'l2' else 0.0\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "\n",
    "        print(f\"Trial {trial.number}: n_layers={n_layers}, units={units}, activation={activation}, \"\n",
    "              f\"dropout_rate={dropout_rate}, apply_batch_norm={apply_batch_norm}, optimizer={optimizer_name}, \"\n",
    "              f\"regularization={regularization}, reg_lambda={reg_lambda}, learning_rate={learning_rate}\")\n",
    "    else:\n",
    "        params = trial_or_params\n",
    "        n_layers = params['n_layers']\n",
    "        units = [params[f'n_units_{i}'] for i in range(n_layers)]\n",
    "        activation = params['activation']\n",
    "        dropout_rate = params['dropout_rate']\n",
    "        apply_batch_norm = params['apply_batch_norm']\n",
    "        optimizer_name = params['optimizer']\n",
    "        regularization = params['regularization']\n",
    "        reg_lambda = params['reg_lambda'] if regularization == 'l2' else 0.0\n",
    "        learning_rate = params['learning_rate']\n",
    "\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for unit in units:\n",
    "        model.add(Dense(unit, kernel_regularizer=l2(reg_lambda) if regularization == 'l2' else None))\n",
    "        if activation == 'leaky_relu':\n",
    "            model.add(LeakyReLU())\n",
    "        else:\n",
    "            model.add(Activation('relu'))\n",
    "        if apply_batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(num_labels, activation='sigmoid'))\n",
    "\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'nadam':\n",
    "        optimizer = Nadam(learning_rate=learning_rate)\n",
    "\n",
    "    print(\"Model summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "class OptunaPruningCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, trial: Trial, monitor_metrics: dict):\n",
    "        super().__init__()\n",
    "        self.trial = trial\n",
    "        self.monitor_metrics = monitor_metrics\n",
    "        self.custom_scores = []  # List to store custom scores\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        val_subset_accuracy = logs.get(self.monitor_metrics['subset_accuracy'])\n",
    "        val_loss = logs.get(self.monitor_metrics['val_loss'])\n",
    "        val_hamming_loss = logs.get(self.monitor_metrics['hamming_loss'])\n",
    "        val_mcc = logs.get(self.monitor_metrics['mcc'])\n",
    "        val_f1_score = logs.get(self.monitor_metrics['f1_score'])\n",
    "\n",
    "        if val_subset_accuracy is None or val_loss is None or val_hamming_loss is None or val_mcc is None or val_f1_score is None:\n",
    "            return\n",
    "\n",
    "        # Calculate the custom score\n",
    "        custom_score = custom_objective(val_subset_accuracy, val_loss, val_hamming_loss, val_mcc, val_f1_score)\n",
    "        self.custom_scores.append(custom_score)  # Append custom score to the list\n",
    "        print(f' Epoch {epoch + 1}: Custom Score = {custom_score}')\n",
    "\n",
    "        self.trial.report(custom_score, step=epoch)\n",
    "\n",
    "        if self.trial.should_prune():\n",
    "            self.model.stop_training = True\n",
    "            raise optuna.exceptions.TrialPruned(f\"Trial was pruned at epoch {epoch} due to suboptimal performance.\")\n",
    "\n",
    "\n",
    "# Create an Optuna study with HyperbandPruner\n",
    "pruner = optuna.pruners.HyperbandPruner(min_resource=1, max_resource=50, reduction_factor=3)\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Preprocess the data\n",
    "# apply_yj = False  # You can make this a hyperparameter if you want\n",
    "# apply_scaler = True  # You can make this a hyperparameter if you want\n",
    "# X_processed = preprocess_data(X_selected, apply_yj, apply_scaler)\n",
    "\n",
    "# # First, split the data into 85% training/validation and 15% test\n",
    "# X_temp, X_test, y_temp, y_test = train_test_split(X_processed, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# # Now, split the 85% into 70% training and 15% validation (which is 85% * 0.176 = 15%)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "# First, split the data into 85% training/validation and 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_selected, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Now, split the 85% into 70% training and 15% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Proceed with Optuna for hyperparameter tuning using X_train and X_val\n",
    "\n",
    "# Use the Ask-and-Tell interface for better control\n",
    "for trial_number in range(10):  # Set the number of trials you want to run\n",
    "    trial = study.ask()\n",
    "\n",
    "    batch_size = trial.suggest_int('batch_size', 512, 1024)\n",
    "    #batch_size = trial.suggest_int('batch_size', 512, 1024)\n",
    "    \n",
    "    # Suggest whether to apply Yeo-Johnson transformation and/or scaling\n",
    "    apply_yj = trial.suggest_categorical('apply_yj', [True, False])\n",
    "    apply_scaler = trial.suggest_categorical('apply_scaler', [True, False])\n",
    "\n",
    "    \n",
    "    # Apply preprocessing based on the Optuna suggestions\n",
    "    X_train_processed = preprocess_data(X_train, apply_yj, apply_scaler)\n",
    "    X_val_processed = preprocess_data(X_val, apply_yj, apply_scaler)\n",
    "    X_test_processed = preprocess_data(X_test, apply_yj, apply_scaler)  # Preprocess test data similarly\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"\\nTrial {trial_number}:\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\" - Apply Yeo-Johnson: {apply_yj}\")\n",
    "    print(f\" - Apply Scaler: {apply_scaler}\")\n",
    "    print(f\"X_train_processed shape: {X_train_processed.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val_processed shape: {X_val_processed.shape}\")\n",
    "    print(f\"y_val shape: {y_val.shape}\")\n",
    "\n",
    "    model, optimizer = create_model(trial, input_shape=(X_train_processed.shape[1],), num_labels=y_train.shape[1])\n",
    "\n",
    "    # # Compile the model with standard binary cross-entropy loss\n",
    "    # model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[subset_accuracy, hamming_loss, matthews_correlation, macro_f1_score])\n",
    "    from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "    # Compile the model with BinaryFocalCrossentropy\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss=BinaryFocalCrossentropy(gamma=2.0, from_logits=False),\n",
    "        metrics=[subset_accuracy, hamming_loss, matthews_correlation, macro_f1_score]\n",
    "    )\n",
    "\n",
    "\n",
    "    monitor_metrics = {\n",
    "        'subset_accuracy': 'val_subset_accuracy',\n",
    "        'val_loss': 'val_loss',\n",
    "        'hamming_loss': 'val_hamming_loss',\n",
    "        'mcc': 'val_matthews_correlation',\n",
    "        'f1_score': 'val_macro_f1_score'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        optuna_callback = OptunaPruningCallback(trial, monitor_metrics=monitor_metrics)\n",
    "        history = model.fit(X_train_processed, y_train,\n",
    "                            validation_data=(X_val_processed, y_val),\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=50,\n",
    "                            verbose=1,\n",
    "                            callbacks=[optuna_callback])\n",
    "\n",
    "        val_subset_accuracy = np.max(history.history['val_subset_accuracy'])\n",
    "        val_loss = np.min(history.history['val_loss'])\n",
    "        val_hamming_loss = np.min(history.history['val_hamming_loss'])\n",
    "        val_mcc = np.max(history.history['val_matthews_correlation'])\n",
    "        val_f1_score = np.max(history.history['val_macro_f1_score'])\n",
    "\n",
    "        custom_score = custom_objective(val_subset_accuracy, val_loss, val_hamming_loss, val_mcc, val_f1_score)\n",
    "        study.tell(trial, custom_score)\n",
    "\n",
    "    except optuna.exceptions.TrialPruned as e:\n",
    "        study.tell(trial, state=optuna.trial.TrialState.PRUNED)\n",
    "        print(str(e))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Save the study to a file\n",
    "os.makedirs('optuna_trials_file', exist_ok=True)\n",
    "study.trials_dataframe().to_csv(os.path.join('optuna_trials_file', 'optuna_study_results.csv'))\n",
    "\n",
    "# Combined plotting for training and validation metrics\n",
    "metrics_pairs = [\n",
    "    ('subset_accuracy', 'val_subset_accuracy'),\n",
    "    ('hamming_loss', 'val_hamming_loss'),\n",
    "    ('matthews_correlation', 'val_matthews_correlation'),\n",
    "    ('macro_f1_score', 'val_macro_f1_score'),\n",
    "    ('loss', 'val_loss')\n",
    "]\n",
    "\n",
    "for train_metric, val_metric in metrics_pairs:\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[train_metric], label=f'Training {train_metric}')\n",
    "    plt.plot(history.history[val_metric], label=f'Validation {val_metric}')\n",
    "    plt.title(f'{train_metric} vs. {val_metric}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(train_metric)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join('optuna_trials_file', f'{train_metric}_vs_{val_metric}_history.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the custom score over epochs\n",
    "plt.figure()\n",
    "plt.plot(optuna_callback.custom_scores, label='Custom Score')\n",
    "plt.title('Custom Score over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Custom Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join('optuna_trials_file', 'custom_score_history.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
