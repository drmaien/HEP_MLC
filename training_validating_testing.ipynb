{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# Custom metrics with correct casting to avoid float/int issues\n",
    "def subset_accuracy(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.cast(tf.reduce_all(tf.equal(tf.round(y_true), tf.round(y_pred)), axis=1), tf.float32))\n",
    "\n",
    "def hamming_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.cast(tf.not_equal(y_true, tf.round(y_pred)), tf.float32))\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    tn = tf.reduce_sum(tf.cast((1 - y_true) * (1 - y_pred), tf.float32))\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32))\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32))\n",
    "\n",
    "    numerator = tp * tn - fp * fn\n",
    "    denominator = tf.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + tf.keras.backend.epsilon())\n",
    "\n",
    "def macro_f1_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.round(y_pred), tf.float32)\n",
    "\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32), axis=0)\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32), axis=0)\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "\n",
    "    return tf.reduce_mean(f1)\n",
    "\n",
    "# Set the best hyperparameters\n",
    "best_params = {\n",
    "    'apply_yj': True,\n",
    "    'apply_scaler': True,\n",
    "    'batch_size': 781, \n",
    "    'n_layers': 3,\n",
    "    'n_units_0': 875,\n",
    "    'n_units_1': 938,\n",
    "    'n_units_2': 402,\n",
    "    'activation': 'relu',\n",
    "    'dropout_rate': 0.11698557003292169,\n",
    "    'apply_batch_norm': True,\n",
    "    'optimizer': 'adam', # could try 'nadam'\n",
    "    'regularization': None,\n",
    "    'reg_lambda': 0.05,\n",
    "    'learning_rate': 0.00026320814416562444\n",
    "}\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('final_dataset.tsv', sep='\\t')\n",
    "feature_cols = ['mH2', 'mHD', 'mAD', 'mHDp', 'alpha', 'L2', 'L8', 'vs', 'm22sq']\n",
    "label_cols = ['valid_BFB', 'valid_Uni', 'valid_STU', 'valid_Higgs']\n",
    "\n",
    "X_selected = df[feature_cols].copy()\n",
    "y = df[label_cols]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(X, apply_yj=False, apply_scaler=False):\n",
    "    if apply_yj:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        X = pt.fit_transform(X)\n",
    "    if apply_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "# Split data into training, validation, and test sets (70%, 15%, 15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_selected, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "\n",
    "# Apply preprocessing based on the best parameters\n",
    "X_train_processed = preprocess_data(X_train, best_params['apply_yj'], best_params['apply_scaler'])\n",
    "X_val_processed = preprocess_data(X_val, best_params['apply_yj'], best_params['apply_scaler'])\n",
    "X_test_processed = preprocess_data(X_test, best_params['apply_yj'], best_params['apply_scaler'])\n",
    "\n",
    "# Define the model\n",
    "def create_model(params, input_shape, num_labels):\n",
    "    n_layers = params['n_layers']\n",
    "    units = [params[f'n_units_{i}'] for i in range(n_layers)]\n",
    "    activation = params['activation']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    apply_batch_norm = params['apply_batch_norm']\n",
    "    regularization = params['regularization']\n",
    "    reg_lambda = params['reg_lambda'] if regularization == 'l2' else 0.0\n",
    "    learning_rate = params['learning_rate']\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for unit in units:\n",
    "        model.add(Dense(unit, kernel_regularizer=l2(reg_lambda) if regularization == 'l2' else None))\n",
    "        model.add(Activation(activation))\n",
    "        if apply_batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(num_labels, activation='sigmoid'))\n",
    "\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif params['optimizer'] == 'nadam':\n",
    "        optimizer = Nadam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss=BinaryFocalCrossentropy(gamma=2.0, from_logits=False),\n",
    "        metrics=[subset_accuracy, hamming_loss, matthews_correlation, macro_f1_score]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = create_model(best_params, input_shape=(X_train_processed.shape[1],), num_labels=y_train.shape[1])\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'Final_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Callbacks for early stopping and model checkpointing\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint_path = os.path.join(output_dir, 'best_model.keras')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_processed, y_train,\n",
    "    validation_data=(X_val_processed, y_val),\n",
    "    batch_size=best_params['batch_size'],\n",
    "    epochs=70,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Save the full model and training history\n",
    "model.save(os.path.join(output_dir, 'final_model.keras'))\n",
    "\n",
    "# Save training history as JSON\n",
    "history_dict = history.history\n",
    "with open(os.path.join(output_dir, 'training_history.json'), 'w') as f:\n",
    "    json.dump(history_dict, f)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_metrics = model.evaluate(X_test_processed, y_test)\n",
    "y_pred_proba = model.predict(X_test_processed)\n",
    "y_pred_classes = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Print Subset Accuracy on Test Set\n",
    "test_subset_accuracy = subset_accuracy(y_test, y_pred_classes).numpy()\n",
    "print(f\"Subset Accuracy on Test Set: {test_subset_accuracy:.4f}\")\n",
    "\n",
    "# Print and save classification reports for each label\n",
    "y_combined_test = y_test.to_numpy()\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "    print(f\"Classification report for {label}:\")\n",
    "    report = classification_report(y_combined_test[:, i], y_pred_classes[:, i])\n",
    "    print(report)\n",
    "    \n",
    "    with open(os.path.join(output_dir, f'classification_report_{label}.txt'), 'w') as f:\n",
    "        f.write(f\"Classification report for {label}:\\n\")\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"\\nClassification report for {label} (focusing on class 0):\")\n",
    "    report_class0 = classification_report(y_combined_test[:, i], y_pred_classes[:, i], labels=[0], target_names=['class_0'], zero_division=1)\n",
    "    print(report_class0)\n",
    "    \n",
    "    with open(os.path.join(output_dir, f'classification_report_{label}_class0.txt'), 'w') as f:\n",
    "        f.write(f\"Classification report for {label} (focusing on class 0):\\n\")\n",
    "        f.write(report_class0)\n",
    "\n",
    "# Plot confusion matrices for each label\n",
    "descriptive_titles = {\n",
    "    'valid_BFB': 'BfB',\n",
    "    'valid_Uni': 'PU',\n",
    "    'valid_STU': 'STU',\n",
    "    'valid_Higgs': 'Higgs'\n",
    "}\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "    cm = confusion_matrix(y_combined_test[:, i], y_pred_classes[:, i])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(f'Confusion Matrix for {descriptive_titles[label]}\\nin Combined Test Set', fontsize=16)\n",
    "    plt.xlabel('Predicted', fontsize=14)\n",
    "    plt.ylabel('Actual', fontsize=14)\n",
    "    plt.xticks(np.arange(2) + 0.5, ['0', '1'], rotation=0, fontsize=12)\n",
    "    plt.yticks(np.arange(2) + 0.5, ['0', '1'], rotation=0, fontsize=12)\n",
    "    \n",
    "    for (j, k), value in np.ndenumerate(cm):\n",
    "        plt.text(k + 0.5, j + 0.5, f'{value}', ha='center', va='center',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='black', facecolor='white'),\n",
    "                 fontsize=14, weight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'confusion_matrix_{label}_combined.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Plot training and validation metrics\n",
    "metrics_pairs = [\n",
    "    ('subset_accuracy', 'val_subset_accuracy'),\n",
    "    ('hamming_loss', 'val_hamming_loss'),\n",
    "    ('matthews_correlation', 'val_matthews_correlation'),\n",
    "    ('macro_f1_score', 'val_macro_f1_score'),\n",
    "    ('loss', 'val_loss')\n",
    "]\n",
    "\n",
    "for train_metric, val_metric in metrics_pairs:\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[train_metric], label=f'Training {train_metric}')\n",
    "    plt.plot(history.history[val_metric], label=f'Validation {val_metric}')\n",
    "    plt.title(f'{train_metric} vs. {val_metric}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(train_metric)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, f'{train_metric}_vs_{val_metric}_history.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Analysis on specific combinations of labels\n",
    "unique_combinations = np.unique(y_combined_test, axis=0)\n",
    "for combination in unique_combinations:\n",
    "    mask = np.all(y_combined_test == combination, axis=1)\n",
    "    subset_true = y_combined_test[mask]\n",
    "    subset_pred = y_pred_classes[mask]\n",
    "    \n",
    "    print(f\"\\nPerformance for combination {combination}:\")\n",
    "    report_combination = classification_report(subset_true, subset_pred, target_names=label_cols)\n",
    "    print(report_combination)\n",
    "    \n",
    "    with open(os.path.join(output_dir, f'classification_report_combination_{\"_\".join(map(str, combination))}.txt'), 'w') as f:\n",
    "        f.write(f\"Performance for combination {combination}:\\n\")\n",
    "        f.write(report_combination)\n",
    "    \n",
    "    # Save misclassified instances for each combination\n",
    "    misclassified_mask = ~np.all(subset_true == subset_pred, axis=1)\n",
    "    misclassified_indices = np.where(mask)[0][misclassified_mask]\n",
    "    misclassified_data = pd.DataFrame(X_test_processed[misclassified_indices], columns=feature_cols)\n",
    "    misclassified_data[label_cols] = y_combined_test[misclassified_indices]\n",
    "    misclassified_data['predicted_' + '_'.join(label_cols)] = [tuple(row) for row in y_pred_classes[misclassified_indices]]\n",
    "    misclassified_data.to_csv(os.path.join(output_dir, f'misclassified_combination_{\"_\".join(map(str, combination))}.csv'), index=False)\n",
    "    print(f\"Misclassified data for combination {combination} saved.\")\n",
    "\n",
    "print(\"All evaluation outputs have been saved to the output directory.\")\n",
    "\n",
    "\n",
    "###################################\n",
    "# ADDITIONAL EVALUATIONS\n",
    "###################################\n",
    "\n",
    "# 1. Powerset-based evaluation:\n",
    "# Treat each unique combination as a class and measure macro F1 across these combination-classes.\n",
    "print(\"\\n=== Powerset-Based Evaluation ===\")\n",
    "# Map each combination to an integer class\n",
    "combo_to_class = {tuple(c): idx for idx, c in enumerate(unique_combinations)}\n",
    "\n",
    "y_true_combos = [combo_to_class[tuple(row)] for row in y_combined_test]\n",
    "y_pred_combos = [combo_to_class[tuple(row)] for row in y_pred_classes]\n",
    "\n",
    "powerset_precision = precision_score(y_true_combos, y_pred_combos, average='macro', zero_division=1)\n",
    "powerset_recall = recall_score(y_true_combos, y_pred_combos, average='macro', zero_division=1)\n",
    "powerset_f1 = f1_score(y_true_combos, y_pred_combos, average='macro', zero_division=1)\n",
    "\n",
    "print(f\"Powerset Macro-F1: {powerset_f1:.4f}\")\n",
    "print(f\"Powerset Macro-Precision: {powerset_precision:.4f}\")\n",
    "print(f\"Powerset Macro-Recall: {powerset_recall:.4f}\")\n",
    "\n",
    "powerset_report = classification_report(y_true_combos, y_pred_combos, zero_division=1)\n",
    "print(\"Powerset Classification Report:\\n\", powerset_report)\n",
    "with open(os.path.join(output_dir, 'powerset_classification_report.txt'), 'w') as f:\n",
    "    f.write(\"Powerset Classification Report:\\n\")\n",
    "    f.write(powerset_report)\n",
    "\n",
    "\n",
    "# 2. Distributional comparisons (Jensen-Shannon Divergence):\n",
    "print(\"\\n=== Distributional Comparison ===\")\n",
    "# Compute empirical distribution of true combinations\n",
    "unique_combo_indices, true_counts = np.unique(y_true_combos, return_counts=True)\n",
    "unique_combo_indices_pred, pred_counts = np.unique(y_pred_combos, return_counts=True)\n",
    "\n",
    "# Create full distributions (some combos might not appear in predictions or vice versa)\n",
    "num_combos = len(unique_combinations)\n",
    "true_dist = np.zeros(num_combos, dtype=np.float32)\n",
    "pred_dist = np.zeros(num_combos, dtype=np.float32)\n",
    "\n",
    "for idx, count in zip(unique_combo_indices, true_counts):\n",
    "    true_dist[idx] = count\n",
    "for idx, count in zip(unique_combo_indices_pred, pred_counts):\n",
    "    pred_dist[idx] = count\n",
    "\n",
    "true_dist /= np.sum(true_dist)\n",
    "pred_dist /= np.sum(pred_dist)\n",
    "\n",
    "# JS divergence between distributions (JS is symmetric and sqrt(JS) is a metric)\n",
    "js_div = jensenshannon(true_dist, pred_dist)**2\n",
    "print(f\"Jensen-Shannon Divergence between true and predicted combination distributions: {js_div:.6f}\")\n",
    "with open(os.path.join(output_dir, 'distribution_comparison.txt'), 'w') as f:\n",
    "    f.write(f\"Jensen-Shannon Divergence: {js_div}\\n\")\n",
    "\n",
    "\n",
    "# 3. Conditional Probability Checks:\n",
    "print(\"\\n=== Conditional Probability Checks ===\")\n",
    "# For each pair of labels i, j, check P(y_j=1 | y_i=1) in true and predicted sets\n",
    "def conditional_probabilities(y_true, y_pred):\n",
    "    # y_true and y_pred are binary arrays of shape (N, L)\n",
    "    L = y_true.shape[1]\n",
    "    results = []\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            if i == j:\n",
    "                continue\n",
    "            mask_i = (y_true[:, i] == 1)\n",
    "            true_cond = np.mean(y_true[mask_i, j]) if np.sum(mask_i) > 0 else 0.0\n",
    "\n",
    "            mask_i_pred = (y_pred[:, i] == 1)\n",
    "            pred_cond = np.mean(y_pred[mask_i_pred, j]) if np.sum(mask_i_pred) > 0 else 0.0\n",
    "\n",
    "            results.append((label_cols[i], label_cols[j], true_cond, pred_cond))\n",
    "    return results\n",
    "\n",
    "cond_probs = conditional_probabilities(y_combined_test, y_pred_classes)\n",
    "for (li, lj, t_c, p_c) in cond_probs:\n",
    "    print(f\"P({lj}=1|{li}=1) True: {t_c:.3f}, Pred: {p_c:.3f}\")\n",
    "\n",
    "with open(os.path.join(output_dir, 'conditional_probabilities.txt'), 'w') as f:\n",
    "    f.write(\"Conditional Probabilities (P(label_j=1|label_i=1))\\n\")\n",
    "    for (li, lj, t_c, p_c) in cond_probs:\n",
    "        f.write(f\"{li}->{lj}: True={t_c:.3f}, Pred={p_c:.3f}\\n\")\n",
    "\n",
    "print(\"\\nAdditional evaluation outputs have been saved to the output directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
